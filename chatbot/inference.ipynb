{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.317482Z",
     "start_time": "2018-08-05T15:25:51.392907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Anaconda3/anaconda3/envs/env1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, TimeDistributed\n",
    "import pickle\n",
    "# import nltk\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "# from keras.utils import preprocessing\n",
    "# from model.textGenModel import TextGenModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.651626Z",
     "start_time": "2018-08-05T15:26:13.322792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16841888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Did you change your hair?No.You might wanna think about itI missed you.It says here you exposed yourself to a group of freshmen girls.It was a bratwurst.  I was eating lunch.With the teeth of your zi'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2=''\n",
    "with open(INPUT_DIR + 'movie_data/movie_conversations_temp.txt', 'r') as f:\n",
    "    corpus_text_2 = f.readlines()\n",
    "#     print(corpus_text_2[0])\n",
    "    for c in corpus_text_2:\n",
    "        str2 = ' '.join([str2, c])\n",
    "\n",
    "print(len(str2))\n",
    "str2[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.662577Z",
     "start_time": "2018-08-05T15:26:13.655900Z"
    }
   },
   "outputs": [],
   "source": [
    "# constant token and params for our models\n",
    "START_TOKEN = \"EOS\"\n",
    "END_TOKEN = \"EOS\"\n",
    "UNKNOWN_TOKEN = \"UNK\"\n",
    "PADDING_TOKEN = \"PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.688137Z",
     "start_time": "2018-08-05T15:26:13.667257Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_words_mappings(tokenized_sentences, vocabulary_size):\n",
    "    # Using NLTK\n",
    "    #frequence = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    #vocab = frequence.most_common(vocabulary_size)\n",
    "\n",
    "    # Using basic counter\n",
    "    counter = collections.Counter(itertools.chain(*tokenized_sentences))\n",
    "    vocab = counter.most_common(vocabulary_size)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    # Add padding for index 0\n",
    "    index_to_word.insert(0, START_TOKEN)\n",
    "    index_to_word.insert(1, PADDING_TOKEN)\n",
    "    # Append unknown token (with index = vocabulary size + 1)\n",
    "    index_to_word.append(UNKNOWN_TOKEN)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "    \n",
    "    return index_to_word, word_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.703044Z",
     "start_time": "2018-08-05T15:26:13.693960Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "sent_max_len = 20\n",
    "\n",
    "# hidden_size = 512\n",
    "# hidden_size = 128\n",
    "# embedding_size = 128\n",
    "# batch_size = 64\n",
    "max_sent_len = 20\n",
    "embedding_size = 32\n",
    "stateful = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:13.714033Z",
     "start_time": "2018-08-05T15:26:13.707174Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus_text3 = str2\n",
    "# corpus_tokens_tmp=[]\n",
    "# for i in range(0, 16000000, 100000):\n",
    "#     print(i)\n",
    "#     tmp_corpus = [token.orth_ for token in nlp(corpus_text3[i:i + 100000])]\n",
    "#     corpus_tokens_tmp = corpus_tokens_tmp + tmp_corpus\n",
    "# print('Example tokenized excerpt: {}'.format(corpus_tokens_tmp[10:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:15.224709Z",
     "start_time": "2018-08-05T15:26:13.717545Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('corpus_token_tmp.dat', 'rb') as p:\n",
    "    corpus_tokens_tmp = pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:18.666910Z",
     "start_time": "2018-08-05T15:26:15.229673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3825494\n",
      "Vocabulary size = 50003\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_tokens_tmp))\n",
    "vocabulary_size = 50000\n",
    "index_to_word, word_to_index = get_words_mappings(\n",
    "                                        [corpus_tokens_tmp], #cause a list of sentences is expected\n",
    "                                        vocabulary_size)\n",
    "vocabulary_size = len(index_to_word)\n",
    "print(\"Vocabulary size = \" + str(vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:19.192548Z",
     "start_time": "2018-08-05T15:26:18.675471Z"
    }
   },
   "outputs": [],
   "source": [
    "df_word_to_index = pd.DataFrame.from_dict(word_to_index, orient='index')\n",
    "index_to_word_dict = dict([(i,w) for w,i in word_to_index.items()])\n",
    "df_index_to_word = pd.DataFrame.from_dict(index_to_word_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:19.288899Z",
     "start_time": "2018-08-05T15:26:19.203979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0\n",
      "veg                 30696\n",
      "ups'ville</u>!That  44129\n",
      "29                  13046\n",
      "Okay?So             44131\n",
      "meat.\"Yeah          48256\n",
      "     0\n",
      "0  EOS\n",
      "1  PAD\n",
      "2    .\n",
      "3    ,\n",
      "4    I\n"
     ]
    }
   ],
   "source": [
    "print(df_word_to_index.head())\n",
    "print(df_index_to_word.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:21.030233Z",
     "start_time": "2018-08-05T15:26:19.293996Z"
    }
   },
   "outputs": [],
   "source": [
    "df_word_to_index.to_csv('w2i.csv', encoding='utf-8')\n",
    "df_index_to_word.to_csv('i2w.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:21.847843Z",
     "start_time": "2018-08-05T15:26:21.035045Z"
    }
   },
   "outputs": [],
   "source": [
    "df_word_to_index = pd.read_csv('w2i.csv', encoding='utf-8', index_col =0)\n",
    "df_index_to_word = pd.read_csv('i2w.csv', encoding='utf-8', index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:32.143147Z",
     "start_time": "2018-08-05T15:26:21.857094Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index_dict = df_word_to_index.to_dict(orient='index')\n",
    "index_to_word_dict = df_index_to_word.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:32.163350Z",
     "start_time": "2018-08-05T15:26:32.148419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nan: {'0': 46979}, 'veg': {'0': 30696}, \"ups'ville</u>!That\": {'0': 44129}, '29': {'0': 13046}, 'Okay?So': {'0': 44131}}\n",
      "{0: {'0': 'EOS'}, 1: {'0': 'PAD'}, 2: {'0': '.'}, 3: {'0': ','}, 4: {'0': 'I'}}\n"
     ]
    }
   ],
   "source": [
    "print({k: word_to_index_dict[k] for k in list(word_to_index_dict)[:5]})\n",
    "print({k: index_to_word_dict[k] for k in list(index_to_word_dict)[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:36.248397Z",
     "start_time": "2018-08-05T15:26:32.168431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, None, 32)     1600096     enc_input[0][0]                  \n",
      "                                                                 dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm (LSTM)                 [(None, 32), (None,  8320        shared_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, None, 32), ( 8320        shared_emb[1][0]                 \n",
      "                                                                 enc_lstm[0][1]                   \n",
      "                                                                 enc_lstm[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_output (Dense)              (None, None, 50003)  1650099     dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,266,835\n",
      "Trainable params: 3,266,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder_model_loaded = load_model('/Users/ashwins/Scripts/chatbot/full_model_new_epoch_3.h5')\n",
    "autoencoder_model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:36.263856Z",
     "start_time": "2018-08-05T15:26:36.253505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0xb497d8668>\n",
      "<keras.engine.input_layer.InputLayer object at 0xb497d86d8>\n",
      "<keras.layers.embeddings.Embedding object at 0xb497d8860>\n",
      "<keras.layers.recurrent.LSTM object at 0xb497d8c88>\n",
      "<keras.layers.recurrent.LSTM object at 0xb497d8940>\n",
      "<keras.layers.core.Dense object at 0xb497d8d30>\n"
     ]
    }
   ],
   "source": [
    "for l in autoencoder_model_loaded.layers:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:37.874568Z",
     "start_time": "2018-08-05T15:26:36.269712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.training.Model object at 0xb497d85c0>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Anaconda3/anaconda3/envs/env1/lib/python3.5/site-packages/keras/engine/saving.py:270: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model = load_model('/Users/ashwins/Scripts/chatbot/enc_model_new_epoch_2.h5')\n",
    "print(encoder_model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:37.900286Z",
     "start_time": "2018-08-05T15:26:37.879379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, None)              0         \n",
      "_________________________________________________________________\n",
      "shared_emb (Embedding)       (None, None, 32)          1600096   \n",
      "_________________________________________________________________\n",
      "enc_lstm (LSTM)              [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 1,608,416\n",
      "Trainable params: 1,608,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc_inputs = autoencoder_model_loaded.get_layer('enc_input').output\n",
    "# enc_inputs = Input(shape=(None,), name='enc_input')\n",
    "# enc_emb = autoencoder_model_loaded.get_layer('shared_emb').get_output_at(0)\n",
    "enc_lstm = autoencoder_model_loaded.get_layer('enc_lstm').output\n",
    "\n",
    "encoder_model_loaded = Model(enc_inputs, enc_lstm)\n",
    "encoder_model_loaded.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:37.910211Z",
     "start_time": "2018-08-05T15:26:37.904670Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:38.951167Z",
     "start_time": "2018-08-05T15:26:37.917740Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_inp = autoencoder_model_loaded.get_layer('dec_input').output\n",
    "dec_emb = autoencoder_model_loaded.get_layer('shared_emb')(dec_inp)\n",
    "dec_lstm_layer = autoencoder_model_loaded.get_layer('dec_lstm')\n",
    "\n",
    "dec_lstm,  decoder_state_output_h, decoder_state_output_c = dec_lstm_layer(dec_emb, initial_state=decoder_input_states)\n",
    "\n",
    "decoder_output_states = [decoder_state_output_h, decoder_state_output_c]\n",
    "\n",
    "dec_output = autoencoder_model_loaded.get_layer('dec_output')(dec_lstm)\n",
    "\n",
    "decoder_model_loaded = Model([dec_inp] + decoder_input_states , [dec_output] + decoder_output_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:38.998871Z",
     "start_time": "2018-08-05T15:26:38.963944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, None, 32)     1600096     dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, None, 32), ( 8320        shared_emb[2][0]                 \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_output (Dense)              (None, None, 50003)  1650099     dec_lstm[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,258,515\n",
      "Trainable params: 3,258,515\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:39.009569Z",
     "start_time": "2018-08-05T15:26:39.003912Z"
    }
   },
   "outputs": [],
   "source": [
    "# index_to_word_dict = dict([(i,w) for w,i in word_to_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:39.027262Z",
     "start_time": "2018-08-05T15:26:39.015500Z"
    }
   },
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"UNK\"\n",
    "PAD_TOKEN = \"PAD\"\n",
    "EOS_TOKEN = \"EOS\"\n",
    "\n",
    "unknown_token_idx = word_to_index_dict.get(UNKNOWN_TOKEN).get('0')\n",
    "pad_token_idx = word_to_index_dict.get(PAD_TOKEN).get('0')\n",
    "eos_token_idx = word_to_index_dict.get(EOS_TOKEN).get('0')\n",
    "\n",
    "max_sent_len = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sequence new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:39.143648Z",
     "start_time": "2018-08-05T15:26:39.033119Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    op, states_value0, states_value1 = encoder_model_loaded.predict(input_seq)\n",
    "    print(np.array(states_value0).shape, np.array(states_value1).shape)\n",
    "#     states_value0 = np.reshape(np.array(states_value0), (20, -1))\n",
    "#     states_value1 = np.reshape(np.array(states_value1), (20, -1))\n",
    "#     print(states_value0.shape, states_value1.shape)\n",
    "    \n",
    "#     states_value0 = states_value0.tolist()\n",
    "#     states_value1 = states_value1.tolist()\n",
    "#     print(states_value1)\n",
    "    states_value = [states_value0, states_value1]\n",
    "#     states_value = [states_value0, states_value1]\n",
    "#     states_value = [np.argmax(states_value0), np.argmax(states_value1)]\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_sent_len))\n",
    "#     target_seq = np.zeros(( max_sent_len, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index_dict[\"PAD\"].get('0')\n",
    "    print(target_seq.shape)\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i=1\n",
    "    while not stop_condition :\n",
    "        output_tokens, h, c = decoder_model_loaded.predict(\n",
    "            [target_seq] + states_value)\n",
    "        print(\"output_tokens shape: \", output_tokens.shape)\n",
    "        print(\"output tokens  : \",output_tokens)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print(\"max index : \", sampled_token_index)\n",
    "        \n",
    "        print(\"predicted word : \", index_to_word_dict.get(sampled_token_index).get('0'))\n",
    "        sampled_word = index_to_word_dict.get(sampled_token_index).get('0')\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        if i == max_sent_len-1 or sampled_token_index == unknown_token_idx or sampled_token_index == eos_token_idx:\n",
    "            stop_condition = True\n",
    "#             print(\"stpping: \", len(decoded_sentence))\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "#         target_seq.append(sampled_token_index )\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "#         target_seq = np.zeros((1, max_sent_len))\n",
    "        target_seq[0, i] = sampled_token_index\n",
    "#         target_seq[i, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        print(\"dec state shapes: \", np.array(h).shape, np.array(c).shape)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decode sequence old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:39.181014Z",
     "start_time": "2018-08-05T15:26:39.153770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence_old(input_seq):\n",
    "    print(input_seq.shape)\n",
    "    # Encode the input as state vectors.\n",
    "    op, states_value0, states_value1 = encoder_model_loaded.predict(input_seq)\n",
    "    \n",
    "    print(op.shape)\n",
    "#     print(\"enc op : \", index_to_word_dict.get(np.argmax(op[0,-1,:])).get('0'))\n",
    "    print(np.array(states_value0).shape, np.array(states_value1).shape)\n",
    "#     print(states_value1)\n",
    "    states_value = [states_value0, states_value1]\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.ones((1, max_sent_len))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_to_index_dict.get(\"EOS\").get('0')\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "#     while not stop_condition :\n",
    "    output_tokens, h, c = decoder_model_loaded.predict(\n",
    "        [target_seq] + states_value)\n",
    "    print(\"output_tokens shape: \", output_tokens.shape)\n",
    "    print(\"output tokens  : \",output_tokens)\n",
    "    \n",
    "    # Sample a token\n",
    "    for i in range(20):\n",
    "        sampled_token_index = np.argmax(output_tokens[0, i, :])\n",
    "        print(\"predicted word : \", index_to_word_dict.get(sampled_token_index,\"none\").get('0'))\n",
    "#     sampled_word = index_to_word_dict[sampled_token_index]\n",
    "        decoded_sentence += ' ' + index_to_word_dict.get(sampled_token_index,\"none\").get('0')\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "#     target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # Update states\n",
    "#     states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T15:26:42.270617Z",
     "start_time": "2018-08-05T15:26:39.198072Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Did', ' ', '?']\n",
      "(3,)\n",
      "[227   7  20]\n",
      "[[227   7  20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]]\n",
      "eval data shape (1, 20)\n",
      "(1, 32) (1, 32)\n",
      "(1, 20)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]\n",
      "  [1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]\n",
      "  [1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]\n",
      "  ...\n",
      "  [1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]\n",
      "  [1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]\n",
      "  [1.6017710e-05 3.8429911e-04 1.5718726e-05 ... 1.5836396e-05\n",
      "   1.5566311e-05 3.4192318e-04]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[1.1039033e-05 1.9094812e-03 1.0687839e-05 ... 1.0818558e-05\n",
      "   1.0516942e-05 1.5767681e-03]\n",
      "  [6.1420951e-06 5.4672942e-03 5.8861629e-06 ... 5.9843187e-06\n",
      "   5.7551429e-06 4.2523118e-03]\n",
      "  [6.1420951e-06 5.4672942e-03 5.8861629e-06 ... 5.9843187e-06\n",
      "   5.7551429e-06 4.2523118e-03]\n",
      "  ...\n",
      "  [6.1420951e-06 5.4672942e-03 5.8861629e-06 ... 5.9843187e-06\n",
      "   5.7551429e-06 4.2523118e-03]\n",
      "  [6.1420951e-06 5.4672942e-03 5.8861629e-06 ... 5.9843187e-06\n",
      "   5.7551429e-06 4.2523118e-03]\n",
      "  [6.1420951e-06 5.4672942e-03 5.8861629e-06 ... 5.9843187e-06\n",
      "   5.7551429e-06 4.2523118e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.2377613e-06 6.5059266e-03 5.0125200e-06 ... 5.1093170e-06\n",
      "   4.8904412e-06 5.0276997e-03]\n",
      "  [5.1263660e-06 6.6481922e-03 4.9048044e-06 ... 5.0008798e-06\n",
      "   4.7840781e-06 5.1314631e-03]\n",
      "  [5.1054162e-06 6.6751447e-03 4.8845786e-06 ... 4.9805622e-06\n",
      "   4.7640501e-06 5.1512467e-03]\n",
      "  ...\n",
      "  [5.1054162e-06 6.6751447e-03 4.8845786e-06 ... 4.9805622e-06\n",
      "   4.7640501e-06 5.1512467e-03]\n",
      "  [5.1054162e-06 6.6751447e-03 4.8845786e-06 ... 4.9805622e-06\n",
      "   4.7640501e-06 5.1512467e-03]\n",
      "  [5.1054162e-06 6.6751447e-03 4.8845786e-06 ... 4.9805622e-06\n",
      "   4.7640501e-06 5.1512467e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1012789e-06 6.6804332e-03 4.8805900e-06 ... 4.9765731e-06\n",
      "   4.7600874e-06 5.1551708e-03]\n",
      "  [5.1004854e-06 6.6814390e-03 4.8798256e-06 ... 4.9758082e-06\n",
      "   4.7593262e-06 5.1559266e-03]\n",
      "  [5.1003212e-06 6.6816611e-03 4.8796696e-06 ... 4.9756491e-06\n",
      "   4.7591666e-06 5.1560961e-03]\n",
      "  ...\n",
      "  [5.1002830e-06 6.6817091e-03 4.8796296e-06 ... 4.9756136e-06\n",
      "   4.7591307e-06 5.1561305e-03]\n",
      "  [5.1002830e-06 6.6817091e-03 4.8796296e-06 ... 4.9756136e-06\n",
      "   4.7591307e-06 5.1561305e-03]\n",
      "  [5.1002830e-06 6.6817091e-03 4.8796296e-06 ... 4.9756131e-06\n",
      "   4.7591307e-06 5.1561305e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002735e-06 6.6817193e-03 4.8796164e-06 ... 4.9756045e-06\n",
      "   4.7591197e-06 5.1561408e-03]\n",
      "  [5.1002685e-06 6.6817226e-03 4.8796164e-06 ... 4.9756022e-06\n",
      "   4.7591197e-06 5.1561431e-03]\n",
      "  [5.1002662e-06 6.6817221e-03 4.8796164e-06 ... 4.9756018e-06\n",
      "   4.7591197e-06 5.1561431e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9756022e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output_tokens shape:  (1, 20, 50003)\n",
      "output tokens  :  [[[5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  ...\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755995e-06\n",
      "   4.7591197e-06 5.1561436e-03]\n",
      "  [5.1002667e-06 6.6817226e-03 4.8796164e-06 ... 4.9755999e-06\n",
      "   4.7591197e-06 5.1561436e-03]]]\n",
      "max index :  4\n",
      "predicted word :  I\n",
      "dec state shapes:  (1, 32) (1, 32)\n",
      "output is :   I I I I I I I I I I I I I I I I I I I\n"
     ]
    }
   ],
   "source": [
    "test_str = \"Did  ?\" #you change your hair\n",
    "# test_str = \"Hey there?\"\n",
    "# text_eval =\"Did you change your hair?\"\n",
    "eval_data = np.array([word_to_index_dict.get(str(w), word_to_index_dict.get(UNKNOWN_TOKEN).get('0')).get('0') \n",
    "              for w in nlp(test_str)])\n",
    "print([str(w) for w in nlp(test_str)])\n",
    "print(eval_data.shape)\n",
    "print(eval_data)\n",
    "eval_data.resize(max_sent_len)\n",
    "eval_data = np.reshape(eval_data, (-1, max_sent_len))\n",
    "# eval_data = np.reshape(eval_data, (max_sent_len, -1))\n",
    "print(eval_data)\n",
    "print(\"eval data shape\", eval_data.shape)\n",
    "op = decode_sequence(eval_data)\n",
    "\n",
    "print(\"output is : \", op)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "123px",
    "width": "162px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
