{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:40:03.082055Z",
     "start_time": "2018-08-12T10:39:49.545913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Anaconda3/anaconda3/envs/env1/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "INPUT_DIR = \"/Users/ashwins/Scripts/chatbot/\"\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, TimeDistributed\n",
    "import pickle\n",
    "# import nltk\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "# from keras.utils import preprocessing\n",
    "# from model.textGenModel import TextGenModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:40:03.142077Z",
     "start_time": "2018-08-12T10:40:03.116686Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:40:03.186860Z",
     "start_time": "2018-08-12T10:40:03.176601Z"
    }
   },
   "outputs": [],
   "source": [
    "# constant token and params for our models\n",
    "START_TOKEN = \"EOS\"\n",
    "END_TOKEN = \"EOS\"\n",
    "UNKNOWN_TOKEN = \"UNK\"\n",
    "PADDING_TOKEN = \"PAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:40:03.228083Z",
     "start_time": "2018-08-12T10:40:03.221763Z"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary_size = 22285\n",
    "sent_max_len = 20\n",
    "\n",
    "# hidden_size = 512\n",
    "hidden_size = 128\n",
    "embedding_size = 128\n",
    "batch_size = 64\n",
    "stateful = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:40:04.727187Z",
     "start_time": "2018-08-12T10:40:03.261812Z"
    }
   },
   "outputs": [],
   "source": [
    "import load_train_data\n",
    "import models.load_ae_model as ae\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T11:22:18.696648Z",
     "start_time": "2018-08-12T11:22:18.468612Z"
    }
   },
   "outputs": [],
   "source": [
    "import models.load_ae_model_regsn as reg\n",
    "import models.load_ae_model_td as td\n",
    "import models.load_ae_model_td_regsn as tdreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:41:57.326200Z",
     "start_time": "2018-08-12T10:40:04.842934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "df_lines :                                                   dialogue     l_no\n",
      "0                                            They do not!    L1045\n",
      "1                                             They do to!    L1044\n",
      "2                                              I hope so.     L985\n",
      "3                                               She okay?     L984\n",
      "4                                               Let's go.     L925\n",
      "5                                                     Wow     L924\n",
      "6          Okay -- you're gonna need to learn how to lie.     L872\n",
      "7                                                      No     L871\n",
      "8       I'm kidding.  You know how sometimes you just ...     L870\n",
      "9                        Like my fear of wearing pastels?     L869\n",
      "10                                        The \"real you\".     L868\n",
      "11                                       What good stuff?     L867\n",
      "12      I figured you'd get to the good stuff eventually.     L866\n",
      "13      Thank God!  If I had to hear one more story ab...     L865\n",
      "14      Me.  This endless ...blonde babble. I'm like, ...     L864\n",
      "15                                             What crap?     L863\n",
      "16                            do you listen to this crap?     L862\n",
      "17                                                  No...     L861\n",
      "18      Then Guillermo says, \"If you go any lighter, y...     L860\n",
      "19                          You always been this selfish?     L699\n",
      "20                                                    But     L698\n",
      "21                        Then that's all you had to say.     L697\n",
      "22                                            Well, no...     L696\n",
      "23          You never wanted to go out with 'me, did you?     L695\n",
      "24                                                 I was?     L694\n",
      "25      I looked for you back at the party, but you al...     L693\n",
      "26                                                   Tons     L663\n",
      "27                                      Have fun tonight?     L662\n",
      "28                   I believe we share an art instructor     L578\n",
      "29                                     You know Chastity?     L577\n",
      "...                                                   ...      ...\n",
      "304683  The only reports of enemy activity have come f...  L666460\n",
      "304684      Our runners bare his dispatches, do they not?  L666385\n",
      "304685  Er, called Noggs, Sir Actual name is Norris-Ne...  L666384\n",
      "304686  What's that strange name the newspaper chap's ...  L666383\n",
      "304687               But will they make good use of them?  L666252\n",
      "304688  Yes. I see you've issued each of them with a M...  L666251\n",
      "304689                  Right.  Bombardier, to me please.  L666502\n",
      "304690  Well, fed or hungry, Pulleine wants them in po...  L666501\n",
      "304691  Well, my horses are feeding, as you may observ...  L666500\n",
      "304692   How quickly can you move your artillery forward?  L666499\n",
      "304693                                               Yes.  L666498\n",
      "304694                                            Stuart?  L666497\n",
      "304695  Keep steady. You're the best shots of the Twen...  L666576\n",
      "304696  Choose your targets men. That's right Watch th...  L666575\n",
      "304697  ft could be you flatter yourself CoghilL It's ...  L666327\n",
      "304698    Well that one. The one who keeps looking at me.  L666326\n",
      "304699                                         Which one?  L666325\n",
      "304700  Do you think she might be interested in  someone?  L666324\n",
      "304701  Well that's typical of Her Majesty's army. App...  L666264\n",
      "304702  Um. There are rumours that my Lord Chelmsford ...  L666263\n",
      "304703  Lighting COGHILL' 5 cigar: Our good Colonel Du...  L666262\n",
      "304704  So far only their scouts. But we have had repo...  L666522\n",
      "304705  And I assure you, you do not In fact I'd be ob...  L666521\n",
      "304706  Well I assure you, Sir, I have no desire to cr...  L666520\n",
      "304707  I think Chelmsford wants a good man on the bor...  L666372\n",
      "304708  Lord Chelmsford seems to want me to stay back ...  L666371\n",
      "304709  I'm to take the Sikali with the main column to...  L666370\n",
      "304710                           Your orders, Mr Vereker?  L666369\n",
      "304711  Good ones, yes, Mr Vereker. Gentlemen who can ...  L666257\n",
      "304712  Colonel Durnford... William Vereker. I hear yo...  L666256\n",
      "\n",
      "[304713 rows x 2 columns]\n",
      "\n",
      "\n",
      "df_convos :             end    start\n",
      "0         L199     L198\n",
      "1         L208     L207\n",
      "2         L277     L276\n",
      "3         L281     L280\n",
      "4         L364     L363\n",
      "5         L366     L365\n",
      "6         L368     L367\n",
      "7         L576     L575\n",
      "8         L578     L577\n",
      "9         L663     L662\n",
      "10        L861     L860\n",
      "11        L925     L924\n",
      "12        L985     L984\n",
      "13       L1045    L1044\n",
      "14        L580     L579\n",
      "15        L660     L659\n",
      "16        L953     L952\n",
      "17        L395     L394\n",
      "18        L397     L396\n",
      "19        L593     L592\n",
      "20        L760     L759\n",
      "21        L165     L164\n",
      "22        L320     L319\n",
      "23        L530     L529\n",
      "24        L543     L542\n",
      "25        L602     L601\n",
      "26        L656     L655\n",
      "27        L983     L982\n",
      "28       L1022    L1021\n",
      "29       L1052    L1051\n",
      "...        ...      ...\n",
      "38061  L665841  L665840\n",
      "38062  L665862  L665861\n",
      "38063  L665874  L665873\n",
      "38064  L665905  L665904\n",
      "38065  L665958  L665957\n",
      "38066  L666030  L666029\n",
      "38067  L666042  L666041\n",
      "38068  L666211  L666210\n",
      "38069  L666215  L666214\n",
      "38070  L666222  L666221\n",
      "38071  L665652  L665651\n",
      "38072  L665886  L665885\n",
      "38073  L665901  L665900\n",
      "38074  L666150  L666149\n",
      "38075  L666159  L666158\n",
      "38076  L666161  L666160\n",
      "38077  L666167  L666166\n",
      "38078  L666169  L666168\n",
      "38079  L665993  L665992\n",
      "38080  L666463  L666462\n",
      "38081  L666250  L666249\n",
      "38082  L666358  L666357\n",
      "38083  L666484  L666483\n",
      "38084  L666488  L666487\n",
      "38085  L666252  L666251\n",
      "38086  L666461  L666460\n",
      "38087  L666486  L666485\n",
      "38088  L666547  L666546\n",
      "38089  L666576  L666575\n",
      "38090  L666257  L666256\n",
      "\n",
      "[38091 rows x 2 columns]\n",
      "16841888\n",
      "Example tokenized excerpt: ['think', 'about', 'itI', 'missed', 'you', '.', 'It', 'says', 'here', 'you']\n",
      "3825494\n",
      "Vocabulary size = 50003\n",
      "                                            dialogue  l_no  l_no_int   end  \\\n",
      "0                                  That I'm used to.   L77        77   L78   \n",
      "1                                       Tempestuous?  L108       108  L109   \n",
      "2  Mandella, eat.  Starving yourself is a very sl...  L129       129  L130   \n",
      "3                                       What's this?  L131       131  L132   \n",
      "4  I realize that the men of this fine institutio...  L133       133  L134   \n",
      "\n",
      "  start  \n",
      "0   L77  \n",
      "1  L108  \n",
      "2  L129  \n",
      "3  L131  \n",
      "4  L133  \n",
      "                                          dialogue_x l_no_x  l_no_int_x   end  \\\n",
      "0                                  That I'm used to.    L77          77   L78   \n",
      "1                                       Tempestuous?   L108         108  L109   \n",
      "2  Mandella, eat.  Starving yourself is a very sl...   L129         129  L130   \n",
      "3                                       What's this?   L131         131  L132   \n",
      "4  I realize that the men of this fine institutio...   L133         133  L134   \n",
      "\n",
      "  start                                         dialogue_y l_no_y  l_no_int_y  \n",
      "0   L77  Yeah, but these guys have never seen a horse. ...    L78          78  \n",
      "1  L108  No ... I believe \"heinous bitch\" is the term u...   L109         109  \n",
      "2  L129                                     Just a little.   L130         130  \n",
      "3  L131                                 An attempted slit.   L132         132  \n",
      "4  L133        But imagine the things he'd say during sex.   L134         134  \n",
      "(38091, 4)\n",
      "\n",
      "\n",
      "df_train_inputs.shape :  (38091, 4)\n",
      "\n",
      "\n",
      "df_train_inputs.head() :      end                                              input  \\\n",
      "0   L78                              [70, 4, 2074, 318, 8]   \n",
      "1  L109                                            [50002]   \n",
      "2  L130  [50002, 511, 41908, 264, 21, 9, 154, 1182, 118...   \n",
      "3  L132                                     [42, 2689, 29]   \n",
      "4  L134  [4, 907, 17, 6, 345, 14, 29, 319, 4692, 43, 15...   \n",
      "\n",
      "                                              output start  \n",
      "0  [157, 66, 207, 306, 24, 114, 305, 9, 1159, 115...   L77  \n",
      "1  [89, 4, 204, 18786, 732, 21, 6, 2874, 318, 375...  L108  \n",
      "2                                      [197, 9, 126]  L129  \n",
      "3                                  [749, 8450, 6992]  L131  \n",
      "4        [79, 958, 6, 185, 39, 3630, 110, 1313, 710]  L133  \n",
      "\n",
      "\n",
      "encoder_input_data.shape :  (38080, 20)\n",
      "\n",
      "\n",
      "encoder_input_data[:5] :  [[   70     4  2074   318     8     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [50002     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [50002   511 41908   264    21     9   154  1182   118     8   415     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [   42  2689    29     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [    4   907    17     6   345    14    29   319  4692    43 15611  8598\n",
      "     66   929   264    72     5    59    27    36]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "decoder_input_data.shape :  (38080, 20)\n",
      "\n",
      "\n",
      "decoder_input_data[:5] :  [[  157    66   207   306    24   114   305     9  1159   115    48  5178\n",
      "    142     8 16665 24256     0     0     0     0]\n",
      " [   89     4   204 18786   732    21     6  2874   318   375  1399     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  197     9   126     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  749  8450  6992     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [   79   958     6   185    39  3630   110  1313   710     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "\n",
      "\n",
      "length :  503\n",
      "\n",
      "\n",
      "decoder_target_data.shape :  (38080, 20, 1)\n",
      "\n",
      "\n",
      "decoder_target_data[:5] :  [[[   66]\n",
      "  [  207]\n",
      "  [  306]\n",
      "  [   24]\n",
      "  [  114]\n",
      "  [  305]\n",
      "  [    9]\n",
      "  [ 1159]\n",
      "  [  115]\n",
      "  [   48]\n",
      "  [ 5178]\n",
      "  [  142]\n",
      "  [    8]\n",
      "  [16665]\n",
      "  [24256]\n",
      "  [  157]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]\n",
      "\n",
      " [[    4]\n",
      "  [  204]\n",
      "  [18786]\n",
      "  [  732]\n",
      "  [   21]\n",
      "  [    6]\n",
      "  [ 2874]\n",
      "  [  318]\n",
      "  [  375]\n",
      "  [ 1399]\n",
      "  [   89]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]\n",
      "\n",
      " [[    9]\n",
      "  [  126]\n",
      "  [  197]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]\n",
      "\n",
      " [[ 8450]\n",
      "  [ 6992]\n",
      "  [  749]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]\n",
      "\n",
      " [[  958]\n",
      "  [    6]\n",
      "  [  185]\n",
      "  [   39]\n",
      "  [ 3630]\n",
      "  [  110]\n",
      "  [ 1313]\n",
      "  [  710]\n",
      "  [   79]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]]\n",
      "Example train input sentence: [  70    4 2074  318    8    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "and related output = [[   66]\n",
      " [  207]\n",
      " [  306]\n",
      " [   24]\n",
      " [  114]\n",
      " [  305]\n",
      " [    9]\n",
      " [ 1159]\n",
      " [  115]\n",
      " [   48]\n",
      " [ 5178]\n",
      " [  142]\n",
      " [    8]\n",
      " [16665]\n",
      " [24256]\n",
      " [  157]\n",
      " [    0]\n",
      " [    0]\n",
      " [    0]\n",
      " [    0]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(load_train_data)\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = load_train_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:41:57.382374Z",
     "start_time": "2018-08-12T10:41:57.368608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38080, 20)\n",
      "(38080, 20)\n",
      "(38080, 20, 1)\n",
      "[[  70    4 2074  318    8    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[[  157    66   207   306    24   114   305     9  1159   115    48  5178\n",
      "    142     8 16665 24256     0     0     0     0]]\n",
      "[[[   66]\n",
      "  [  207]\n",
      "  [  306]\n",
      "  [   24]\n",
      "  [  114]\n",
      "  [  305]\n",
      "  [    9]\n",
      "  [ 1159]\n",
      "  [  115]\n",
      "  [   48]\n",
      "  [ 5178]\n",
      "  [  142]\n",
      "  [    8]\n",
      "  [16665]\n",
      "  [24256]\n",
      "  [  157]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]\n",
      "  [    0]]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "print(encoder_input_data[:1])\n",
    "print(decoder_input_data[:1])\n",
    "print(decoder_target_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:44:49.108915Z",
     "start_time": "2018-08-12T10:44:48.268851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "shared_emb (Embedding)       (None, 1, 32)             1600096   \n",
      "_________________________________________________________________\n",
      "enc_lstm (LSTM)              [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 1,608,416\n",
      "Trainable params: 1,608,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, 1, 32)        1600096     enc_input[0][0]                  \n",
      "                                                                 dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm (LSTM)                 [(None, 32), (None,  8320        shared_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, 1, 32), (Non 8320        shared_emb[1][0]                 \n",
      "                                                                 enc_lstm[0][1]                   \n",
      "                                                                 enc_lstm[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_output (Dense)              (None, 1, 50003)     1650099     dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,266,835\n",
      "Trainable params: 3,266,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50003\n",
    "\n",
    "importlib.reload(ae)\n",
    "auto_encoder, encoder = ae.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:45:28.973323Z",
     "start_time": "2018-08-12T10:45:27.989004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "shared_emb (Embedding)       (None, 20, 32)            1600096   \n",
      "_________________________________________________________________\n",
      "enc_lstm (LSTM)              [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 1,608,416\n",
      "Trainable params: 1,608,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, 20, 32)       1600096     enc_input[0][0]                  \n",
      "                                                                 dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm (LSTM)                 [(None, 32), (None,  8320        shared_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, 20, 32), (No 8320        shared_emb[1][0]                 \n",
      "                                                                 enc_lstm[0][1]                   \n",
      "                                                                 enc_lstm[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_output (Dense)              (None, 20, 1)        33          dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,616,769\n",
      "Trainable params: 1,616,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reg)\n",
    "auto_encoder_r, encoder_r = reg.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:55:16.607801Z",
     "start_time": "2018-08-12T10:55:15.679989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "shared_emb (Embedding)       (None, 20, 32)            1600096   \n",
      "_________________________________________________________________\n",
      "enc_lstm (LSTM)              [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 1,608,416\n",
      "Trainable params: 1,608,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, 20, 32)       1600096     enc_input[0][0]                  \n",
      "                                                                 dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm (LSTM)                 [(None, 32), (None,  8320        shared_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, 20, 32), (No 8320        shared_emb[1][0]                 \n",
      "                                                                 enc_lstm[0][1]                   \n",
      "                                                                 enc_lstm[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 50003)    1650099     dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,266,835\n",
      "Trainable params: 3,266,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(td)\n",
    "auto_encoder_td, encoder_td = td.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T11:22:27.909931Z",
     "start_time": "2018-08-12T11:22:26.776364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "shared_emb (Embedding)       (None, 20, 32)            1600096   \n",
      "_________________________________________________________________\n",
      "enc_lstm (LSTM)              [(None, 32), (None, 32),  8320      \n",
      "=================================================================\n",
      "Total params: 1,608,416\n",
      "Trainable params: 1,608,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (InputLayer)          (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "shared_emb (Embedding)          (None, 20, 32)       1600096     enc_input[0][0]                  \n",
      "                                                                 dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm (LSTM)                 [(None, 32), (None,  8320        shared_emb[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, 20, 32), (No 8320        shared_emb[1][0]                 \n",
      "                                                                 enc_lstm[0][1]                   \n",
      "                                                                 enc_lstm[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 20, 1)        33          dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,616,769\n",
      "Trainable params: 1,616,769\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(tdreg)\n",
    "auto_encoder_td_r, encoder_td_r = tdreg.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T11:10:08.700028Z",
     "start_time": "2018-08-12T10:55:39.953189Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Anaconda3/anaconda3/envs/env1/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer dec_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'enc_lstm_4/while/Exit_3:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'enc_lstm_4/while/Exit_4:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 214s 53ms/step - loss: 10.3461 - val_loss: 8.7691\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 223s 56ms/step - loss: 7.2678 - val_loss: 7.0338\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 222s 56ms/step - loss: 6.6488 - val_loss: 6.9923\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 197s 49ms/step - loss: 6.5862 - val_loss: 6.9898\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "opt = Adam(lr=0.001)\n",
    "auto_encoder_td.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt)\n",
    "\n",
    "model_json = auto_encoder_td.to_json()\n",
    "with open(\"full_model_td_new_arch\", \"w\") as f:\n",
    "    f.write(json.dumps(json.loads((model_json))))\n",
    "\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "for i in range(4):\n",
    "    \n",
    "        \n",
    "#     if i==8:\n",
    "#         opt = Adam(lr=0.00001)\n",
    "#         model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
    "        \n",
    "    model_name = 'full_model_td_new_epoch_' + str(i) + '.h5'\n",
    "    auto_encoder_td.save(model_name)\n",
    "    print('saved model')\n",
    "    \n",
    "    enc_name = 'enc_model_td_new_epoch_' + str(i) + '.h5'\n",
    "    encoder_td.save(enc_name)\n",
    "    print('saved enc model')\n",
    "    weights_path = ''.join(['enc_model_td_new_weights_', \n",
    "                        \"base_voc_{}_epoch_{}.hdf5\".format(vocabulary_size, \n",
    "                                                          i)])\n",
    "    encoder_td.save_weights(weights_path)\n",
    "    print('saved enc weights')\n",
    "    # export model weights\n",
    "    weights_path = ''.join(['full_model_td_new_weights_', \n",
    "                        \"base_voc_{}_epoch_{}.hdf5\".format(vocabulary_size, \n",
    "                                                          i)])\n",
    "    auto_encoder_td.save_weights(weights_path)\n",
    "    print('saved weights')\n",
    "    \n",
    "    if i==3:\n",
    "        opt = Adam(lr=0.0001)\n",
    "        auto_encoder_td.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt)\n",
    "        \n",
    "    auto_encoder_td.fit([encoder_input_data[:5000], decoder_input_data[:5000]], decoder_target_data[:5000], batch_size=batch_size, epochs=1, validation_split=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-12T10:16:49.723113Z",
     "start_time": "2018-08-12T10:16:22.734170Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Anaconda3/anaconda3/envs/env1/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer dec_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'enc_lstm_9/while/Exit_3:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'enc_lstm_9/while/Exit_4:0' shape=(?, 32) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 69468240.3200 - val_loss: 71810720.0960\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 6s 2ms/step - loss: 69388372.9920 - val_loss: 71799783.6160\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 69974015.1040 - val_loss: 71788903.4880\n",
      "saved model\n",
      "saved enc model\n",
      "saved enc weights\n",
      "saved weights\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "opt = Adam(lr=0.001)\n",
    "auto_encoder_r.compile(loss='mean_squared_error',\n",
    "              optimizer=opt)\n",
    "\n",
    "model_json = auto_encoder_r.to_json()\n",
    "with open(\"full_model_new_arch\", \"w\") as f:\n",
    "    f.write(json.dumps(json.loads((model_json))))\n",
    "\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "for i in range(4):\n",
    "    \n",
    "        \n",
    "#     if i==8:\n",
    "#         opt = Adam(lr=0.00001)\n",
    "#         model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
    "        \n",
    "    model_name = 'full_model_r_new_epoch_' + str(i) + '.h5'\n",
    "    auto_encoder_r.save(model_name)\n",
    "    print('saved model')\n",
    "    \n",
    "    enc_name = 'enc_model_r_new_epoch_' + str(i) + '.h5'\n",
    "    encoder_r.save(enc_name)\n",
    "    print('saved enc model')\n",
    "    weights_path = ''.join(['enc_model_r_new_weights_', \n",
    "                        \"base_voc_{}_epoch_{}.hdf5\".format(vocabulary_size, \n",
    "                                                          i)])\n",
    "    encoder_r.save_weights(weights_path)\n",
    "    print('saved enc weights')\n",
    "    # export model weights\n",
    "    weights_path = ''.join(['full_model_r_new_weights_', \n",
    "                        \"base_voc_{}_epoch_{}.hdf5\".format(vocabulary_size, \n",
    "                                                          i)])\n",
    "    auto_encoder_r.save_weights(weights_path)\n",
    "    print('saved weights')\n",
    "\n",
    "    if i==3:\n",
    "        break\n",
    "        opt = Adam(lr=0.0001)\n",
    "        auto_encoder_r.compile(loss='mean_squared_error',\n",
    "              optimizer=opt)\n",
    "    \n",
    "    auto_encoder_r.fit([encoder_input_data[:5000], decoder_input_data[:5000]], decoder_target_data[:5000], batch_size=batch_size, epochs=1, validation_split=0.2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
